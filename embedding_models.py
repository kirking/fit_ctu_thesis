# -*- coding: utf-8 -*-
"""Embedding_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oYhpE9_wd4XzgtHl6PxXOSdWl7mbIISs
"""

# Commented out IPython magic to ensure Python compatibility.
import networkx as nx
import pandas as pd
import numpy as np
import os
import random

import stellargraph as sg
from stellargraph.data import BiasedRandomWalk
from stellargraph.data import EdgeSplitter
from stellargraph.mapper import GraphSAGELinkGenerator,GraphSAGENodeGenerator
from stellargraph.layer import GraphSAGE, link_classification
from stellargraph.data import UniformRandomWalk
from stellargraph.data import UnsupervisedSampler
from stellargraph.core.graph import StellarDiGraph

from sklearn.model_selection import train_test_split

from tensorflow import keras
from sklearn import preprocessing, feature_extraction, model_selection
from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
from sklearn.metrics import accuracy_score

from stellargraph import globalvar

from stellargraph import datasets
from IPython.display import display, HTML

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from stellargraph.mapper import GraphSAGENodeGenerator
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from stellargraph.utils import plot_history
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping


from stellargraph.mapper import (
    CorruptedGenerator,
    FullBatchNodeGenerator,
    GraphSAGENodeGenerator,
    HinSAGENodeGenerator,
    ClusterNodeGenerator,
)
from stellargraph import StellarGraph
from stellargraph.layer import DeepGraphInfomax, GraphSAGE

from stellargraph import datasets
from stellargraph.utils import plot_history

import pandas as pd
from matplotlib import pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from IPython.display import display, HTML

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow.keras import Model
# %matplotlib inline

# -*- coding: utf-8 -*-
"""
Reference implementation of RiWalk.
Author: Xuewei Ma
For more details, refer to the paper:
RiWalk: Fast Structural Node Embedding via Role Identification
ICDM, 2019
"""

import argparse
import json
import time
import RiWalkGraph
from gensim.models import Word2Vec
from gensim.models.keyedvectors import Word2VecKeyedVectors
import networkx as nx
import os
import glob
import logging
import sys

class Sentences(object):
    """
    a wrapper of random walk files to feed to word2vec
    """
    def __init__(self, file_names):
        self.file_names = file_names

    def __iter__(self):
        fs = []
        for file_name in self.file_names:
            fs.append(open(file_name))
        while True:
            flag = 0
            for i, f in enumerate(fs):
                line = f.readline()
                if line != '':
                    flag = 1
                    yield line.split()
            if not flag:
                try:
                    for f in fs:
                        f.close()
                except:
                    pass
                return

class RiWalk:
    def __init__(self, dimensions, window_size, workers, iter, until_k, num_walks, walk_length, flag, without_discount):
        self.dim = dimensions
        self.window_size = window_size
        self.workers = workers
        self.iter_num = iter
        self.until_k = until_k
        self.num_walks = num_walks
        self.walk_length =  walk_length
        self.flag = flag
        self.without_discount = without_discount
        os.system('rm -rf walks/__random_walks_*.txt')

    def learn_embeddings(self):
        """
        learn embeddings from random walks.
        hs:  0:negative sampling 1:hierarchica softmax
        sg:  0:CBOW              1:skip-gram
        """
        
        logging.debug('begin learning embeddings')
        learning_begin_time = time.time()

        walk_files = glob.glob('walks/__random_walks_*.txt')
        sentences = Sentences(walk_files)
        model = Word2Vec(sentences, size=self.dim, window=self.window_size, min_count=0, sg=1, hs=0, workers=self.workers, iter=self.iter_num)

        learning_end_time = time.time()
        logging.debug('done learning embeddings')
        logging.debug('learning_time: {}'.format(learning_end_time - learning_begin_time))
        print('learning_time', learning_end_time - learning_begin_time, flush=True)
        return model.wv

    def read_graph(self):
        logging.debug('begin reading graph')
        read_begin_time = time.time()

        input_file_name = self.args.input
        nx_g = nx.read_edgelist(input_file_name, nodetype=int, create_using=nx.DiGraph())
        for edge in nx_g.edges():
            nx_g[edge[0]][edge[1]]['weight'] = 1
        nx_g = nx_g.to_undirected()

        logging.debug('done reading graph')
        read_end_time = time.time()
        logging.debug('read time: {}'.format(read_end_time - read_begin_time))
        return nx_g

    def preprocess_graph(self, nx_g):
        """
        1. relabel nodes with 0,1,2,3,...,N.
        2. convert graph to adjacency representation as a list of lists.
        """
        logging.debug('begin preprocessing graph')
        preprocess_begin_time = time.time()

        mapping = {_: i for i, _ in enumerate(nx_g.nodes())}
        nx_g = nx.relabel_nodes(nx_g, mapping)
        nx_g = [list(nx_g.neighbors(_)) for _ in range(len(nx_g))]

        logging.info('#nodes: {}'.format(len(nx_g)))
        logging.info('#edges: {}'.format(sum([len(_) for _ in nx_g]) // 2))

        logging.debug('done preprocessing')
        logging.debug('preprocess time: {}'.format(time.time() - preprocess_begin_time))
        return nx_g, mapping

    def learn(self, nx_g, mapping):
        g = RiWalkGraph.RiGraph(nx_g, self.until_k, self.num_walks, self.walk_length, self.workers, self.flag, self.without_discount)

        walk_time, bfs_time, ri_time, walks_writing_time = g.process_random_walks()

        print('walk_time', walk_time / self.workers, flush=True)
        print('bfs_time', bfs_time / self.workers, flush=True)
        print('ri_time', ri_time / self.workers, flush=True)
        print('walks_writing_time', walks_writing_time / self.workers, flush=True)
        
        wv = self.learn_embeddings()

        original_wv = Word2VecKeyedVectors(self.dim)
        original_nodes = list(mapping.keys())
        original_vecs = [wv.word_vec(str(mapping[node])) for node in original_nodes]
        original_wv.add(entities=list(map(str, original_nodes)), weights=original_vecs)
        return original_wv

    def riwalk(self, nx_g):
        # nx_g = self.read_graph()
        read_end_time = time.time()
        nx_g, mapping = self.preprocess_graph(nx_g)
        wv = self.learn(nx_g, mapping)
        return wv, time.time() - read_end_time

class g_sage:

  def __init__(self, in_graph, num_walks = 1, length = 5, batch_size = 50, num_samples = [10, 10], layer_sizes = [50, 50]):
    self.batch_size = batch_size
    self.num_samples = num_samples
    self.layer_sizes = layer_sizes
    nodes = list(in_graph.nodes())
    unsupervised_samples = UnsupervisedSampler(
    in_graph, nodes=nodes, length=length, number_of_walks=num_walks)

    generator = GraphSAGELinkGenerator(in_graph, batch_size, num_samples)
    self.train_gen = generator.flow(unsupervised_samples)

    graphsage = GraphSAGE( layer_sizes=layer_sizes, generator=generator,
                          bias=True, dropout=0.0, normalize="l2")

    self.x_inp, self.x_out = graphsage.in_out_tensors()

    prediction = link_classification( output_dim=1, output_act="sigmoid",
                                     edge_embedding_method="ip")(self.x_out)

    self.model = keras.Model(inputs=self.x_inp, outputs=prediction)

    self.model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.binary_crossentropy,
    metrics=[keras.metrics.binary_accuracy],
    )

  def train(self, epochs = 100, verbose = False):
    es = EarlyStopping(monitor="loss", min_delta=0, patience=20)
    history = self.model.fit(
    self.train_gen,
    epochs=epochs,
    verbose=1,
    use_multiprocessing=True,
    workers=4,
    shuffle=True, callbacks=[es]
    )

    if verbose:
      plot_history(history)

    x_inp_src = self.x_inp[0::2]
    x_out_src = self.x_out[0]

    self.model = keras.Model(inputs=x_inp_src, outputs=x_out_src)

  def save(self, filename):
    self.model.save(filename)

  def load(self, filename):
    self.model = keras.models.load_model(filename)

  def embedding(self, in_graph, f_df):
    node_gen = GraphSAGENodeGenerator(in_graph,
                                      self.batch_size, self.num_samples).flow(f_df.index)
    return self.model.predict(node_gen, workers=4, verbose=1)

class deep_infomax:
  def __init__(self, in_graph, batch_size = 1000, num_samples = [5], layer_sizes = [128]):
    self.batch_size = batch_size
    self.num_samples = num_samples
    self.layer_sizes= layer_sizes
    self.graphsage_generator = GraphSAGENodeGenerator(in_graph, batch_size=batch_size, num_samples=num_samples)
    self.graphsage_model = GraphSAGE(
    layer_sizes = layer_sizes, activations=["relu"], generator=self.graphsage_generator
)
    self.corrupted_generator = CorruptedGenerator(self.graphsage_generator)
    self.generator_flow = self.corrupted_generator.flow(in_graph.nodes())
    self.infomax = DeepGraphInfomax(self.graphsage_model, self.corrupted_generator)
    x_in, x_out = self.infomax.in_out_tensors()
    self.model = Model(inputs=x_in, outputs=x_out)
    self.model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))
    
  def train(self, epochs = 100, verbose = False, patience = 20):
    es = EarlyStopping(monitor="loss", min_delta=0, patience=patience)
    history = self.model.fit(self.generator_flow, epochs=epochs, verbose=0, callbacks=[es])
    if verbose:
      plot_history(history)

    x_emb_in, x_emb_out = self.graphsage_model.in_out_tensors()

    if self.graphsage_generator.num_batch_dims() == 2:
        x_emb_out = tf.squeeze(x_emb_out, axis=0)

    self.emb_model = Model(inputs=x_emb_in, outputs=x_emb_out)
  
  def embedding(self, in_graph):
    generator = GraphSAGENodeGenerator(in_graph, batch_size = self.batch_size,  num_samples = self.num_samples)
    return self.emb_model.predict(generator.flow(in_graph.nodes()))

class low_dim_transformation:
  def TSNE(embedding, in_graph, f_df, n_components = 2):
    trans = TSNE(n_components= n_components)
    emb_transformed = pd.DataFrame(trans.fit_transform(embedding), index=in_graph.nodes())
    emb_transformed["label"] = f_df['topic'].astype("category").cat.codes

    return emb_transformed

  def draw_embedding(transformed_embedding, filename):
    alpha = 0.7

    fig, ax = plt.subplots(figsize=(7, 7))
    ax.scatter(
      transformed_embedding[0],
      transformed_embedding[1],
      c=transformed_embedding["label"].astype("category").cat.codes,
      cmap="jet",
      alpha=alpha,)
    ax.set(aspect="equal", xlabel="$X_1$", ylabel="$X_2$")
    plt.savefig(filename)
    plt.show()

def node2Vec_embedding(graph, labels, dim = 128):
  rw = BiasedRandomWalk(graph)

  walks = rw.run(
    nodes=list(graph.nodes()),  # root nodes
    length=100,  # maximum length of a random walk
    n=10,  # number of random walks per root node
    p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node
    q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node
    )
  
  str_walks = [[str(n) for n in walk] for walk in walks]
  model = Word2Vec(str_walks, size=dim, window=5, min_count=0, sg=1, workers=2, iter=1)
  if labels is None:
    return ( model.wv.vectors)
  node_subjects = np.array(labels).reshape(-1)
  node_ids = model.wv.index2word  # list of node IDs
  node_embeddings = ( model.wv.vectors)  # numpy.ndarray of size number of nodes times embeddings dimensionality
  node_targets = node_subjects[[int(node_id) for node_id in node_ids]]

  return node_embeddings, node_targets


def riwalk_embedding(g_nx, labels, dim=128):
  wv, _ = RiWalk(dim, 10, 10, 5, 4, 80, 10, 'sp', False).riwalk(g_nx) # dimensions, window_size, workers, iter, until_k, num_walks, walk_length, flag, without_discount
  if labels is None:
    return (wv.vectors)
  node_subjects = np.array(labels).reshape(-1)
  node_ids = wv.index2word
  node_embeddings = (wv.vectors)
  node_targets = node_subjects[[int(node_id) for node_id in node_ids]]
  return node_embeddings, node_targets